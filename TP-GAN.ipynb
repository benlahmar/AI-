{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1137f66",
   "metadata": {},
   "source": [
    "\n",
    "<a id='chap-tpgan'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d964576",
   "metadata": {},
   "source": [
    "# Travaux pratiques : Generative Adversarial Networks\n",
    "\n",
    "L’objectif de cette séance de TP est d’illustrer par la pratique le\n",
    "fonctionnement des réseaux de neurones génératifs antagonistes (ou\n",
    "*Generative Adversarial Networks*, GAN). Cette séance est un peu moins\n",
    "guidée que les précédentes, n’hésitez pas à solliciter l’équipe\n",
    "enseignante en cas de difficultés, notamment liées à la programmation en\n",
    "PyTorch.\n",
    "\n",
    "Rappelons pour commencer le principe des réseaux génératifs\n",
    "antagonistes. Ce modèle génératif met en compétition deux réseaux de\n",
    "neurones $ {D} $ et $ {G} $ que l’on appellera par la suite le\n",
    "discriminateur et le générateur, respectivement.\n",
    "\n",
    "**Note :** on trouve parfois dans la litérature une analogie avec la\n",
    "falsification d’œuvres d’art. $ {D} $ est alors appelé le\n",
    "« critique » et $ {G} $ est appelé le « faussaire ».\n",
    "\n",
    "L’objectif de $ {G} $ est de transformer un bruit aléatoire\n",
    "$ z $ en un échantillon $ \\tilde{x} $ le plus similaire possible\n",
    "aux observations réelles $ x \\in \\mathbf{X} $.\n",
    "\n",
    "À l’inverse, l’objectif de $ {D} $ est d’apprendre à reconnaître les\n",
    "« faux » échantillons $ \\tilde{x} $ des vrais observations\n",
    "$ x $.\n",
    "\n",
    "Pour implémenter un tel modèle, commençons par importer quelques\n",
    "bibliothèques et sous-modules utiles de PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd41d68a",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca346da",
   "metadata": {},
   "source": [
    "## Jeux de données jouet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d346d4e9",
   "metadata": {},
   "source": [
    "### Points sur un cercle\n",
    "\n",
    "Dans un premier temps, considérons un jeu de données simple : des points\n",
    "répartis le long du cercle unité. Une façon d’obtenir ces points est\n",
    "d’échantilloner uniformément selon une loi normale et de diviser chaque\n",
    "vecteur par sa norme pour le rendre unitaire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f60ddce",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "X = np.random.randn(100, 2)\n",
    "X /= np.linalg.norm(X, axis=1)[:, None].repeat(2, axis=1)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "plt.scatter(X[:,0], X[:,1])\n",
    "plt.title(\"Nuage de points\") and plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90124da5",
   "metadata": {},
   "source": [
    "On définit ici les paramètres du problème (dimension $ n $ des\n",
    "données d’entrée, ici $ n=2 $, et dimension de l’espace latent). Ils\n",
    "pourront être modifiés par la suite ci-besoin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1fb9a2",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "data_dim = X.shape[-1]\n",
    "hidden_dim = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fb6d10",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Écrire en utilisant l’interface `nn.Sequential` de PyTorch:\n",
    "\n",
    "- un générateur G entièrement connecté à 3 couches entièrement\n",
    "  connectées qui projette un vecteur $ z $ de l’espace latent de\n",
    "  dimension `hidden_dim` vers un échantillon de dimension\n",
    "  `data_dim` (dimension des données d’entrée),  \n",
    "- un discriminateur D entièrement connecté qui, à partir d’un vecteur\n",
    "  de données (réel ou faux) produit en sortie un valeur entre 0 et 1\n",
    "  (obtenue en passant le score dans une sigmoïde).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0ff4c2",
   "metadata": {},
   "source": [
    "### Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c4f5cc",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "generator = nn.Sequential(\n",
    "    nn.Linear(hidden_dim, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, data_dim)\n",
    ")\n",
    "\n",
    "discriminator = nn.Sequential(\n",
    "    nn.Linear(data_dim, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 1),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d91409",
   "metadata": {},
   "source": [
    "Pour créer le jeu d’apprentissage, `torch` dispose d’une fonction bien\n",
    "pratique qui permet de transformer automatiquement une matrice\n",
    "d’observation en `Dataset`. Nous pouvons créer le `DataLoader` dans\n",
    "la foulée :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8c75a4",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "dataset = data.TensorDataset(torch.Tensor(X))\n",
    "dataloader = data.DataLoader(dataset, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8068bfb4",
   "metadata": {},
   "source": [
    "(modifiez le paramètre `batch_size` si jamais vous ne disposez pas de\n",
    "suffisamment de mémoire ou si les calculs sont trop lents)\n",
    "\n",
    "L’apprentissage des poids des réseaux $ \\mathcal{D} $ et\n",
    "$ \\mathcal{G} $ se fait par des mises à jour séparées. Nous aurons\n",
    "donc besoin de deux optimiseurs différents, un qui porte sur les\n",
    "paramètres du générateur et l’autre qui portent sur les paramètres du\n",
    "discriminateur :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a6f94c",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "G_optimizer = torch.optim.Adam(generator.parameters())\n",
    "D_optimizer = torch.optim.Adam(discriminator.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23495faa",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Compléter la boucle d’apprentissage ci-dessous, et notamment le calcul\n",
    "des fonctions de coût pour le générateur et pour le discriminateur. On\n",
    "rappelle que :\n",
    "\n",
    "- le discriminateur cherche à maximiser\n",
    "  $ \\mathcal{L}_D = \\log D(x) + \\log(1- D(\\hat{x})) $ où $ x $ sont\n",
    "  des données réelles et $ \\hat{x} $ des données générées\n",
    "  ($ \\hat{x} = G(z) $), c’est-à-dire que la sortie de la sigmoïde\n",
    "  du discriminateur doit valoir 1 pour les données réelles et 0 pour\n",
    "  les données fausses,  \n",
    "- le générateur cherche à minimiser\n",
    "  $ \\mathcal{L}_G = \\log (1 - D(\\hat{x})) = \\log (1 - D(G(z)) $, c’est\n",
    "  à dire tromper le discriminateur en le poussant à prédire que des\n",
    "  données fausses sont réelles.  \n",
    "\n",
    "\n",
    "L’algorithme d’apprentissage du GAN est donc le suivant :\n",
    "\n",
    "Tant que la convergence n’est pas atteinte\n",
    "\n",
    "1. Tirer un batch $ x $ de données réelles  \n",
    "1. Tirer un bruit aléatoire $ z \\in \\mathcal{N}(0,1) $  \n",
    "1. Générer des fausses données $ G(z) $  \n",
    "1. Calculer la fonction de coût de $ D $ sur les données réelles + fausses  \n",
    "1. Faire un pas d’optimisation de $ D $  \n",
    "1. Calculer la fonction de coût de $ G $ sur les données synthétiques  \n",
    "1. Faire un pas d’optimisation sur $ G $  \n",
    "\n",
    "\n",
    "On rappelle que la méthode `.backward()` permet de rétropropager les\n",
    "gradients d’un tenseur et que `optimizer.step()` permet ensuite de\n",
    "réaliser un pas de descente de gradient (mise à jour des poids).\n",
    "\n",
    "**Attention**: il ne faut pas rétropropager le gradient dans $ G $\n",
    "lorsque vous réalisez une itération sur $ D $. Cela peut se faire à\n",
    "l’aide de la méthode `.detach()` qui permet de signifier à PyTorch que\n",
    "vous n’aurez pas besoin du gradient pour le tenseur concerné."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aad8bad",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b7f428",
   "metadata": {},
   "source": [
    "### Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fab62e",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "for epoch in trange(10000):\n",
    "    for real_data, in dataloader:\n",
    "        bs = len(real_data)\n",
    "\n",
    "        # Échantillonne des codes au hasard dans l'espace latent\n",
    "        z = torch.randn((len(real_data), hidden_dim))\n",
    "\n",
    "        fake_data = generator(z)\n",
    "        predictions = discriminator(fake_data)\n",
    "\n",
    "        # Classe \"faux\" = 0\n",
    "        fake_labels =  torch.zeros(len(fake_data))\n",
    "        # Classe \"vrai\" = 1\n",
    "        true_labels = torch.ones(len(real_data))\n",
    "\n",
    "        # Entraîne le générateur\n",
    "        G_optimizer.zero_grad()\n",
    "        G_loss = torch.log(1 - predictions).mean()\n",
    "        G_loss.backward()\n",
    "        G_optimizer.step()\n",
    "\n",
    "        # Entraîne le discriminateur\n",
    "        D_optimizer.zero_grad()\n",
    "        predictions = discriminator(fake_data.detach())[:,0]\n",
    "        true_predictions = discriminator(real_data)[:,0]\n",
    "        D_loss = 0.5 * (F.binary_cross_entropy(predictions, fake_labels) + F.binary_cross_entropy(true_predictions, true_labels))\n",
    "        D_loss.backward()\n",
    "        D_optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c29a04",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Tirez aléatoirement des vecteurs $ z $ de bruit selon une loi\n",
    "normale. Générez les points associés et visualisez côte à côte le nuage\n",
    "de points réel et le nuage de points synthétique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54059e46",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "z = torch.randn(1000, hidden_dim)\n",
    "with torch.no_grad():\n",
    "    samples = generator(z)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "fig.add_subplot(121)\n",
    "plt.scatter(real_data[:, 0], real_data[:, 1])\n",
    "plt.title(\"Points réels\")\n",
    "fig.add_subplot(122)\n",
    "plt.scatter(samples[:, 0], samples[:, 1])\n",
    "plt.title(\"Points générés\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb1f000",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Colorez les points générés en fonction de la classe (“real” ou “fake”) prédite par le discriminateur. On mettera le seuil à 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f886fc33",
   "metadata": {},
   "source": [
    "### Demi-lunes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b8f2c9",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "*Optionnel (passez cette question si vous êtes pressés)* Remplacez le\n",
    "jeu de données $ \\mathbf{X} $ défini plus haut par les deux\n",
    "demi-lunes ci-dessous et répondez aux mêmes questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372c0ad7",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "X, y = datasets.make_moons(n_samples=1000, noise=0.05)\n",
    "plt.scatter(X[:,0], X[:, 1], c=y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333d2703",
   "metadata": {},
   "source": [
    "### Swiss roll\n",
    "\n",
    "Le *swiss roll* (gâteau roulé) est un jeu de données tridimensionnel\n",
    "formant un plan replié sur lui-même."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449c7e4c",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "X, y = datasets.make_swiss_roll(n_samples=2000, noise=0.05)\n",
    "import mpl_toolkits.mplot3d.axes3d as p3\n",
    "fig = plt.figure()\n",
    "ax = p3.Axes3D(fig)\n",
    "ax.scatter(X[:, 0], X[:,1], X[:,2], c=y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3cc39f",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "*Optionnel (passez cette question si vous êtes pressés*) Répétez l’expérience ci-dessous. Que constatez-vous lorsque vous\n",
    "diminuez la dimensionalité de l’espace latent ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f2577f",
   "metadata": {},
   "source": [
    "## Génération de chiffres manuscrits avec MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eac9069",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "Reprendre le code précédent et l’adapter à la dimensionalité de MNIST.\n",
    "On rappelle qu’une image de MNIST est de dimensions $ 28\\times28 $,\n",
    "c’est-à-dire un vecteur de dimension 784 une fois aplati.\n",
    "\n",
    "Implémenter un GAN **conditionnel** sur MNIST. On utilisera comme\n",
    "vecteur de conditionnement $ y $ la classe du chiffre sous forme de\n",
    "vecteur suivant l’encodage *one-hot*. Visualiser les chiffres obtenus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977e2d6d",
   "metadata": {},
   "source": [
    "## Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653c7cd1",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "train_mnist = MNIST(root=\"./data/\", train=True, download=True, transform=ToTensor())\n",
    "mnist_loader = data.DataLoader(train_mnist, shuffle=True, batch_size=128, num_workers=1)\n",
    "device = \"cuda\" if torch.cuda.is_available else \"cpu\" # Utilise le GPU si disponible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ccbcfe",
   "metadata": {},
   "source": [
    "Définition du générateur et du discriminateur :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d2968b",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "hidden_dim = 100\n",
    "data_dim = 784\n",
    "\n",
    "generator = nn.Sequential(\n",
    "    nn.Linear(hidden_dim + 10, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1024, data_dim),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "discriminator = nn.Sequential(\n",
    "    nn.Linear(data_dim + 10, 64),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.Dropout(),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(64, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "G_optimizer = torch.optim.Adam(generator.parameters(), lr=0.0001)\n",
    "D_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f047f66e",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def plot_images(generator, z, digit=0):\n",
    "    c = torch.zeros((10, 10))\n",
    "    c[:, digit] = 1.\n",
    "    z = torch.concat((z, c), dim=1)\n",
    "    z = z.to(device)\n",
    "    with torch.no_grad():\n",
    "        samples = generator(z)\n",
    "    fig, axis = plt.subplots(1, 10, figsize=(15, 10))\n",
    "    for idx, sample in enumerate(samples):\n",
    "        sample = sample.to(\"cpu\").numpy().reshape(28, 28)\n",
    "        axis[idx].imshow(sample, cmap=\"gray\")\n",
    "        axis[idx].axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86340184",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "# Déplacement des modèles sur GPU si nécessaire\n",
    "generator = generator.to(device)\n",
    "discriminator = discriminator.to(device)\n",
    "\n",
    "z_display = torch.randn(10, hidden_dim)\n",
    "\n",
    "for epoch in trange(100):\n",
    "    for real_data, labels in mnist_loader:\n",
    "        bs = len(real_data)\n",
    "\n",
    "        real_data = real_data.reshape(bs, -1)\n",
    "        labels = F.one_hot(labels)\n",
    "\n",
    "        # Échantillonne des codes au hasard dans l'espace latent\n",
    "        z = torch.randn((len(real_data), hidden_dim))\n",
    "\n",
    "        # Déplacement des données sur GPU si nécessaire\n",
    "        z = z.to(device)\n",
    "        real_data = real_data.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Concaténer le conditionnement à z et aux données\n",
    "        z = torch.concat((z, labels), dim=1)\n",
    "        real_data = torch.concat((real_data, labels), dim=1)\n",
    "\n",
    "        fake_data = generator(z)\n",
    "        fake_data = torch.concat((fake_data, labels), dim=1) # Concatène le conditionnement\n",
    "        predictions = discriminator(fake_data)\n",
    "\n",
    "        # Classe \"faux\" = 0\n",
    "        fake_labels =  torch.zeros(len(fake_data)).to(device)\n",
    "        # Classe \"vrai\" = 1\n",
    "        true_labels = torch.ones(len(real_data)).to(device)\n",
    "\n",
    "        # Entraîne le générateur\n",
    "        G_optimizer.zero_grad()\n",
    "        G_loss = F.binary_cross_entropy(predictions[:,0], true_labels)\n",
    "        G_loss.backward()\n",
    "        G_optimizer.step()\n",
    "\n",
    "        # Entraîne le discriminateur\n",
    "        D_optimizer.zero_grad()\n",
    "        predictions = discriminator(fake_data.detach())[:,0]\n",
    "        true_predictions = discriminator(real_data)[:,0]\n",
    "        D_loss = 0.5 * (F.binary_cross_entropy(predictions, fake_labels) + F.binary_cross_entropy(true_predictions, true_labels))\n",
    "        D_loss.backward()\n",
    "        D_optimizer.step()\n",
    "    tqdm.write(f\"Epoch {epoch}; Discriminator: {D_loss.item():.3f}, Generator: {G_loss.item():.3f}\")\n",
    "    plot_images(generator, z_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbad16b6",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "Cette question est optionnelle et sert d’approfondissement. Remplacez\n",
    "$ G $ et $ D $ définis ci-dessus par des modèles convolutifs. On\n",
    "pourra notamment utiliser à bon escient les modules `Upsample` ou\n",
    "`Conv2DTranspose` pour gérer l’augmentation des dimensions dans le\n",
    "générateur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1297d8",
   "metadata": {},
   "source": [
    "## Correction\n",
    "\n",
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262bb144",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "hidden_dim = 100\n",
    "data_dim = 784\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hidden_dim, 7*7*3)\n",
    "\n",
    "        self.deconvolutional = nn.Sequential(nn.Conv2d(13, 32, kernel_size=3, padding=1),\n",
    "                                             nn.ReLU(),\n",
    "                                             nn.UpsamplingBilinear2d(scale_factor=2),\n",
    "                                             nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "                                             nn.ReLU(),\n",
    "                                             nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "                                             nn.UpsamplingBilinear2d(scale_factor=2),\n",
    "                                             nn.ReLU(),\n",
    "                                             nn.Conv2d(64, 1, kernel_size=1, padding=0),\n",
    "                                            )\n",
    "\n",
    "\n",
    "    def forward(self, z, c):\n",
    "        x = self.linear(z)\n",
    "        x = x.reshape(x.size(0), 3, 7, 7)\n",
    "        c = c[:,:,None,None].repeat(1, 1, 7, 7)\n",
    "        x = torch.concat((x, c), dim=1)\n",
    "        x = self.deconvolutional(x)\n",
    "        x = F.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.convolutional = nn.Sequential(nn.Conv2d(11, 16, kernel_size=3, padding=1, stride=2),\n",
    "                                           nn.LeakyReLU(),\n",
    "                                           nn.Conv2d(16, 32, kernel_size=3, padding=1, stride=2),\n",
    "                                           nn.LeakyReLU(),\n",
    "                                           nn.Conv2d(32, 64, kernel_size=3, padding=1, stride=2),\n",
    "                                           nn.LeakyReLU(),\n",
    "                                           )\n",
    "        self.classifier = nn.Sequential(nn.Flatten(),\n",
    "                                        nn.Linear(64, 1),\n",
    "                                        nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        c = c[:,:,None,None].repeat(1, 1, 28, 28)\n",
    "        x = torch.concat((x, c), dim=1)\n",
    "        x = self.convolutional(x)\n",
    "        x = F.adaptive_avg_pool2d(x, output_size=(1,1))\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "G_optimizer = torch.optim.Adam(generator.parameters(), lr=0.0001)\n",
    "D_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e1cc27",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def plot_images(generator, z, digit=0):\n",
    "   c = torch.zeros((10, 10))\n",
    "   c[:, digit] = 1.\n",
    "   z = z.to(device)\n",
    "   c = c.to(device)\n",
    "   with torch.no_grad():\n",
    "       samples = generator(z, c)\n",
    "   fig, axis = plt.subplots(1, 10, figsize=(15, 10))\n",
    "   for idx, sample in enumerate(samples):\n",
    "       sample = sample.to(\"cpu\").numpy().reshape(28, 28)\n",
    "       axis[idx].imshow(sample, cmap=\"gray\")\n",
    "       axis[idx].axis(\"off\")\n",
    "   plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d619e997",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "# Déplacement des modèles sur GPU si nécessaire\n",
    "generator = generator.to(device)\n",
    "discriminator = discriminator.to(device)\n",
    "\n",
    "z_display = torch.randn(10, hidden_dim)\n",
    "\n",
    "for epoch in trange(100):\n",
    "    for real_data, labels in tqdm(mnist_loader):\n",
    "        bs = len(real_data)\n",
    "\n",
    "        labels = F.one_hot(labels)\n",
    "\n",
    "        # Échantillonne des codes au hasard dans l'espace latent\n",
    "        z = torch.randn((len(real_data), hidden_dim))\n",
    "\n",
    "        # Déplacement des données sur GPU si nécessaire\n",
    "        z = z.to(device)\n",
    "        real_data = real_data.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        fake_data = generator(z, labels)\n",
    "        predictions = discriminator(fake_data, labels)\n",
    "\n",
    "        # Classe \"faux\" = 0\n",
    "        fake_labels =  torch.zeros(len(fake_data)).to(device)\n",
    "        # Classe \"vrai\" = 1\n",
    "        true_labels = torch.ones(len(real_data)).to(device)\n",
    "\n",
    "        # Entraîne le générateur\n",
    "        G_optimizer.zero_grad()\n",
    "        G_loss = F.binary_cross_entropy(predictions[:,0], true_labels)\n",
    "        G_loss.backward()\n",
    "        G_optimizer.step()\n",
    "\n",
    "        # Entraîne le discriminateur\n",
    "        D_optimizer.zero_grad()\n",
    "        predictions = discriminator(fake_data.detach(), labels)[:,0]\n",
    "        true_predictions = discriminator(real_data, labels)[:,0]\n",
    "        D_loss = 0.5 * (F.binary_cross_entropy(predictions, fake_labels) + F.binary_cross_entropy(true_predictions, true_labels))\n",
    "        D_loss.backward()\n",
    "        D_optimizer.step()\n",
    "    tqdm.write(f\"Epoch {epoch}; Discriminator: {D_loss.item():.3f}, Generator: {G_loss.item():.3f}\")\n",
    "    plot_images(generator, z_display)"
   ]
  }
 ],
 "metadata": {
  "date": 1727249104.5243454,
  "filename": "TP9-GAN.rst",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "title": "Travaux pratiques : Generative Adversarial Networks"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
