{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfeba554",
   "metadata": {},
   "source": [
    "# üéì Travaux Pratiques : Auto-Encodeurs (AE) et Auto-Encodeurs Variationnels (VAE)\n",
    "\n",
    "**Auteur :**Benlahmar Habib\n",
    "\n",
    "## Objectifs P√©dagogiques\n",
    "\n",
    "1.  **Illustrer l'encodage** : Comprendre comment un r√©seau neuronal apprend une **repr√©sentation compress√©e** (l'**espace latent** ou code $\\mathbf{z}$) de donn√©es complexes (images).\n",
    "2.  **Comprendre la G√©n√©ration** : Ma√Ætriser le concept du **VAE** qui, gr√¢ce √† la r√©gularisation de l'espace latent (Divergence KL), peut non seulement reconstruire mais aussi **g√©n√©rer** de nouvelles donn√©es plausibles et permettre l'**interpolation s√©mantique**.\n",
    "\n",
    "Nous utiliserons la base de donn√©es **Fashion-MNIST** (images 28x28 en niveaux de gris) avec le framework **PyTorch**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063f8492",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d115ac",
   "metadata": {},
   "source": [
    "## üìå I. Pr√©ambule : Configuration et Chargement des Donn√©es\n",
    "\n",
    "**Note P√©dagogique sur Fashion-MNIST** : Ce jeu de donn√©es est choisi pour sa complexit√© sup√©rieure √† MNIST (chiffres). Les articles de mode pr√©sentent des **textures et des formes plus complexes**, ce qui est plus pertinent pour tester la capacit√© de compression des auto-encodeurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d687b3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "# Utiliser le GPU si disponible (pour acc√©l√©rer les calculs)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Ex√©cution sur {device}\")\n",
    "\n",
    "# Chargement des datasets. ToTensor() convertit l'image en un tenseur [0, 1]\n",
    "train_dataset = FashionMNIST(root='./data/FashionMNIST', download=True, train=True, transform=ToTensor())\n",
    "test_dataset = FashionMNIST(root='./data/FashionMNIST', download=True, train=False, transform=ToTensor())\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# Visualisation des premi√®res images\n",
    "n_images = 5\n",
    "fig = plt.figure()\n",
    "for i, (image, label) in enumerate(train_dataset):\n",
    "    fig.add_subplot(1, n_images + 1, i + 1)\n",
    "    plt.imshow(ToPILImage()(image), cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    if i >= n_images:\n",
    "        break\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543c1b03",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62711a60",
   "metadata": {},
   "source": [
    "## I. Auto-Encodeur (AE) Classique\n",
    "\n",
    "L'AE vise √† minimiser l'erreur de reconstruction entre l'entr√©e $\\mathbf{x}$ et la sortie $\\mathbf{x}'$ en passant par un **goulot d'√©tranglement** (le code latent $\\mathbf{z}$) qui force la compression de l'information.\n",
    "\n",
    "### 1.1. Architecture et Impl√©mentation D√©taill√©e\n",
    "\n",
    "Nous utilisons une architecture **convolutive** pour capturer les caract√©ristiques spatiales des images. La dimension latente sera fix√©e √† `latent_dimension = 10` (soit 784 pixels r√©duits √† seulement 10 valeurs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57604c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dimension = 10\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, latent_dimension):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        # --- ENCODER ---\n",
    "        # Le r√¥le de l'encodeur est d'extraire les caract√©ristiques et de les compresser.\n",
    "        self.encoder = nn.Sequential(\n",
    "            # Conv 1: 28x28x1 -> 14x14x32 (avec stride 2)\n",
    "            nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1), \n",
    "            nn.ReLU(),\n",
    "            # Conv 2: 14x14x32 -> 7x7x64\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(), # Transforme le 7x7x64 = 3136 en vecteur\n",
    "            # Couche finale: 3136 -> latent_dimension (10)\n",
    "            nn.Linear(in_features=64*7*7, out_features=latent_dimension) \n",
    "        )\n",
    "        # --- D√âCODEUR ---\n",
    "        # Le r√¥le du d√©codeur est de \"d√©compresser\" le code z pour reconstruire l'image.\n",
    "        self.decoder_linear = nn.Linear(in_features=latent_dimension, out_features=64*7*7) \n",
    "        \n",
    "        # Note P√©dagogique: ConvTranspose2d (D√©convolution) augmente la r√©solution spatiale\n",
    "        self.decoder = nn.Sequential(\n",
    "            # Conv Transpose 1: 7x7x64 -> 14x14x32\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1), \n",
    "            nn.ReLU(),\n",
    "            # Conv Transpose 2: 14x14x32 -> 28x28x1\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid(), # Force la sortie √† √™tre entre [0, 1] (normalisation)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. Encodage\n",
    "        z = self.encoder(x)\n",
    "        \n",
    "        # 2. D√©codage (√©tape lin√©aire et Reshape)\n",
    "        hat_x = F.relu(self.decoder_linear(z))\n",
    "        # Reconvertir le vecteur en grille pour les convolutions : (batch, 64, 7, 7)\n",
    "        hat_x = hat_x.view(-1, 64, 7, 7) \n",
    "        \n",
    "        # 3. D√©codage (√©tape convolutive)\n",
    "        hat_x = self.decoder(hat_x)\n",
    "        \n",
    "        return hat_x, z # Retourne reconstruction (hat_x) et code latent (z)\n",
    "\n",
    "net = AutoEncoder(latent_dimension)\n",
    "net.to(device);\n",
    "print(\"Mod√®le AE initialis√©.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4099f6",
   "metadata": {},
   "source": [
    "### 1.2. Entra√Ænement\n",
    "\n",
    "**Fonction de co√ªt (Loss)** : Nous utilisons l'Erreur Quadratique Moyenne (MSE) : $\\mathcal{L}(\\mathbf{x}, \\mathbf{x}') = ||\\mathbf{x} - \\mathbf{x}'||^2$. Elle mesure la distance euclidienne entre l'image originale et sa reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6676d7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_dataset, epochs=10, learning_rate=1e-3, batch_size=128, device=device):\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    optimizer = torch.optim.Adam(params=net.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    criterion = nn.MSELoss()\n",
    "    net = net.to(device).train()\n",
    "\n",
    "    t = trange(1, epochs + 1, desc=\"Entra√Ænement de l'Auto-Encodeur\")\n",
    "    for epoch in t:\n",
    "        avg_loss = 0.\n",
    "        for images, _ in tqdm(train_dataloader):\n",
    "            images = images.to(device)\n",
    "            \n",
    "            reconstructions, _ = net(images) \n",
    "            loss = criterion(reconstructions, images)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item()\n",
    "\n",
    "        avg_loss /= len(train_dataloader)\n",
    "        t.set_description(f\"Epoch {epoch}: loss = {avg_loss:.3f}\")\n",
    "    return net\n",
    "\n",
    "# net = train(net, train_dataset, epochs=10) # <-- D√âCOMMENTER POUR LANCER L'ENTRAINEMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc4581f",
   "metadata": {},
   "source": [
    "### 1.3. Visualisation : Reconstruction et D√©bruitage (Denoising)\n",
    "\n",
    "Un AE entra√Æn√© apprend implicitement √† ignorer le bruit, car il n'est pas une caract√©ristique essentielle du jeu de donn√©es. Il peut donc servir de filtre de d√©bruitage (*denoising*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0319f42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonctions utilitaires de visualisation\n",
    "def show_grid(grid, title=\"\", figsize=(12, 6)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.title(title)\n",
    "    # make_grid arrange les images en grille. numpy() et transpose (1, 2, 0) sont n√©cessaires pour matplotlib.\n",
    "    plt.imshow(np.transpose(grid.numpy(), (1, 2, 0)))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "def visualize_reconstructions(net, images, device=device):\n",
    "    net = net.to(device).eval()\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        reconstructions = net(images)[0]\n",
    "        # make_grid arrange les 50 premi√®res images en grille 10x5\n",
    "        image_grid = make_grid(reconstructions[1:51], 10, 5).cpu() \n",
    "        return image_grid\n",
    "\n",
    "# Exemple de jeu de test\n",
    "images, _ = iter(test_dataloader).next()\n",
    "\n",
    "show_grid(make_grid(images[1:51],10,5), title=\"Images Originales de Test\")\n",
    "\n",
    "# Calcul du bruit\n",
    "noise = torch.rand_like(images) - 0.5 \n",
    "noisy_images = torch.clamp(images + 0.5 * noise, 0, 1) # Ajout de bruit et bornage [0, 1]\n",
    "\n",
    "show_grid(make_grid(noisy_images[1:51],10,5), title=\"Images de Test Bruit√©es\")\n",
    "\n",
    "# IMPORTANT: D√âCOMMENTER UNIQUEMENT APR√àS AVOIR ENTRA√éN√â LE MOD√àLE (NET)\n",
    "# show_grid(visualize_reconstructions(net, noisy_images), title=\"Reconstruction (D√©bruitage) par l'Auto-Encodeur\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbb1590",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf8a0b9",
   "metadata": {},
   "source": [
    "## II. Auto-Encodeur Variationnel (VAE)\n",
    "\n",
    "Le VAE r√©sout le probl√®me de **discontinuit√©** de l'espace latent de l'AE en for√ßant cet espace √† suivre une distribution Gaussienne simple ($\\,\\mathcal{N}(\\mathbf{0}, \\mathbf{I})$) gr√¢ce √† une p√©nalit√© de r√©gularisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0464bdf7",
   "metadata": {},
   "source": [
    "### 2.1. Concepts Cl√©s du VAE\n",
    "\n",
    "1.  **Sortie de l'Encodeur** : L'encodeur produit la **moyenne** $\\boldsymbol{\\mu}$ et le **log-variance** $\\log(\\boldsymbol{\\sigma}^2)$ de la distribution latente, et non un simple point $\\mathbf{z}$.\n",
    "2.  **Astuce de Reparam√©trisation** : Le code latent $\\mathbf{z}$ est √©chantillonn√© de mani√®re d√©rivable :\n",
    "    $$\\mathbf{z} = \\boldsymbol{\\mu} + \\boldsymbol{\\sigma} \\odot \\boldsymbol{\\epsilon}, \\quad \\text{o√π } \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$$\n",
    "3.  **Fonction de Co√ªt (ELBO)** : La fonction de co√ªt est la somme de deux termes : la perte de **Reconstruction** et la **Divergence de Kullback-Leibler (KL)** (r√©gularisation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a51e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dimension):\n",
    "        super(Encoder, self).__init__()\n",
    "        # Partie convolutive identique √† l'AE\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        # Sorties s√©par√©es pour les param√®tres de la distribution latente\n",
    "        self.linear_mu = nn.Linear(in_features=64*7*7, out_features=latent_dimension) # Moyenne\n",
    "        self.linear_logvar = nn.Linear(in_features=64*7*7, out_features=latent_dimension) # Log-Variance\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x_mu = self.linear_mu(x)\n",
    "        x_logvar = self.linear_logvar(x) \n",
    "        return x_mu, x_logvar\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    # Le d√©codeur reste inchang√© (identique √† l'AE)\n",
    "    def __init__(self, latent_dimension):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.linear = nn.Linear(in_features=latent_dimension, out_features=64*7*7)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        hat_x = F.relu(self.linear(z))\n",
    "        hat_x = hat_x.view(-1, 64, 7, 7)\n",
    "        hat_x = self.model(hat_x)\n",
    "        return hat_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc64b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.decoder = Decoder(latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent_mu, latent_logvar = self.encoder(x)\n",
    "        # √âtape cruciale : l'√©chantillonnage avec reparam√©trisation\n",
    "        z = self.latent_sample(latent_mu, latent_logvar)\n",
    "        hat_x = self.decoder(z)\n",
    "        return hat_x, latent_mu, latent_logvar # Les 3 sorties sont n√©cessaires pour la loss\n",
    "\n",
    "    def latent_sample(self, mu, logvar):\n",
    "        # L'astuce de reparam√©trisation n'est appliqu√©e qu'√† l'entra√Ænement\n",
    "        if self.training:\n",
    "            # 1. sigma = exp(0.5 * logvar)\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            # 2. epsilon ~ N(0, I)\n",
    "            eps = torch.empty_like(std).normal_()\n",
    "            # 3. z = mu + sigma * epsilon\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            # En inf√©rence, on prend simplement le point le plus probable (la moyenne)\n",
    "            return mu \n",
    "\n",
    "vae = VariationalAutoencoder(latent_dimension)\n",
    "vae.to(device);\n",
    "print(\"Mod√®le VAE initialis√©.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabb95ea",
   "metadata": {},
   "source": [
    "### 2.2. Fonction de Co√ªt du VAE (ELBO)\n",
    "\n",
    "La fonction de co√ªt du VAE est : $\\text{Loss} = \\text{Reconstruction Loss} + \\beta \\times \\text{KL Divergence}$.\n",
    "\n",
    "1.  **Reconstruction Loss (BCE)** : Utilise l'Entropie Crois√©e Binaire (BCE) pour les images.\n",
    "2.  **KL Divergence** : Formule analytique entre $q_{\\phi}(\\mathbf{z}|\\mathbf{x}) = \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\sigma}^2)$ et $p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ :\n",
    "    $$KL = \\frac{1}{2} \\sum_j^d \\bigl ( 1 + \\log((\\sigma_j)^2) - (\\mu_j)^2 - (\\sigma_j)^2 \\bigr)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2febb7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 1.0 # Param√®tre du beta-VAE\n",
    "\n",
    "def vae_loss(hat_x, x, mu, logvar):\n",
    "    # Terme 1: Reconstruction Loss (BCE)\n",
    "    # R√©duction='sum' car on veut la perte totale du batch, non la moyenne par pixel\n",
    "    reconstruction_loss = F.binary_cross_entropy(hat_x.view(-1, 28*28), x.view(-1, 28*28), reduction='sum')\n",
    "    \n",
    "    # Terme 2: KL Divergence Loss\n",
    "    # Formule: -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    # Loss finale\n",
    "    return reconstruction_loss + beta * kl_divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9247c6",
   "metadata": {},
   "source": [
    "### 2.3. Entra√Ænement\n",
    "\n",
    "La fonction d'entra√Ænement est adapt√©e pour utiliser les trois sorties du VAE (`hat_x`, `mu`, `logvar`) dans le calcul de la `vae_loss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c022a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae(net, train_dataset, epochs=10, learning_rate=1e-3, batch_size=128, device=device):\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = torch.optim.Adam(params=net.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    net = net.to(device).train()\n",
    "\n",
    "    t = trange(1, epochs + 1, desc=\"Entra√Ænement du VAE\")\n",
    "    for epoch in t:\n",
    "        avg_loss = 0.\n",
    "        for images, _ in tqdm(train_dataloader):\n",
    "            images = images.to(device)\n",
    "            \n",
    "            reconstructions, latent_mu, latent_logvar = net(images)\n",
    "            loss = vae_loss(reconstructions, images, latent_mu, latent_logvar)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item()\n",
    "\n",
    "        avg_loss /= len(train_dataloader)\n",
    "        t.set_description(f\"Epoch {epoch}: loss = {avg_loss:.3f}\")\n",
    "    return net.to(\"cpu\")\n",
    "\n",
    "# vae = train_vae(vae, train_dataset, epochs=10) # <-- D√âCOMMENTER POUR LANCER L'ENTRAINEMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d111ff1",
   "metadata": {},
   "source": [
    "### 2.4. G√©n√©ration et Interpolation (Le C≈ìur du VAE)\n",
    "\n",
    "Gr√¢ce √† la r√©gularisation de la KL, l'espace latent est structur√©, dense et suit la loi $\\mathcal{N}(\\mathbf{0}, \\mathbf{I})$, permettant la g√©n√©ration par √©chantillonnage al√©atoire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25d4c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: D√âCOMMENTER UNIQUEMENT APR√àS AVOIR ENTRA√éN√â LE MOD√àLE (VAE)\n",
    "\n",
    "# vae.eval()\n",
    "# with torch.no_grad():\n",
    "#     # 1. G√©n√©ration : √âchantillonnage de 100 vecteurs latents N(0, I)\n",
    "#     latent = torch.randn(100, latent_dimension, device=device) \n",
    "#     \n",
    "#     # 2. D√©codage pour obtenir les images synth√©tiques\n",
    "#     fake_images = vae.decoder(latent).cpu() \n",
    "\n",
    "# show_grid(make_grid(fake_images[1:51], 10, 5), title=\"G√©n√©ration de nouvelles images synth√©tiques par le VAE\")\n",
    "\n",
    "# --- Interpolation ---\n",
    "# z1 et z2 sont deux points al√©atoires\n",
    "# z1 = torch.randn(1, latent_dimension, device=device)\n",
    "# z2 = torch.randn(1, latent_dimension, device=device)\n",
    "# n_steps = 10 \n",
    "\n",
    "# fig = plt.figure(figsize=(16, 8))\n",
    "# plt.title(\"Interpolation lin√©aire dans l'espace latent du VAE\")\n",
    "\n",
    "# for idx, alpha in enumerate(np.linspace(0, 1, n_steps + 1)):\n",
    "#     # Interpolation: z = (1 - alpha) * z1 + alpha * z2\n",
    "#     z_interpolated = (1 - alpha) * z1 + alpha * z2\n",
    "#     with torch.no_grad():\n",
    "#         fake_image = vae.decoder(z_interpolated)[0, 0, :, :].cpu().numpy()\n",
    "\n",
    "#     fig.add_subplot(1, n_steps + 1, idx + 1)\n",
    "#     plt.imshow(fake_image, cmap=\"gray\")\n",
    "#     plt.title(f\"alpha = {alpha:0.1f}\")\n",
    "#     plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cad1ea2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956d340a",
   "metadata": {},
   "source": [
    "## III. Approfondissement (Optionnel) : Visualisation de l'Espace Latent\n",
    "\n",
    "Cet exercice utilise **t-SNE** pour projeter les codes latents en 2D et visualiser l'effet de la r√©gularisation KL sur la structure de l'espace.\n",
    "\n",
    "### Question\n",
    "\n",
    "*(Optionnel, pour l'approfondissement)* Pour toutes les images du jeu de test de Fashion-MNIST, calculer le code latent associ√© (on prendra la moyenne $\\boldsymbol{\\mu}$ dans le cas du VAE). Appliquer une r√©duction de dimension non-lin√©aire en utilisant la version de `t-SNE` de `scikit-learn` pour projeter les codes latents dans le plan. Coloriez les points en fonction de leur cat√©gorie.\n",
    "\n",
    "**Que constatez-vous ?**\n",
    "\n",
    "* **AE Classique** : Les clusters de classes sont isol√©s, avec de grands espaces vides entre eux (points incoh√©rents).\n",
    "* **VAE** : Les clusters sont regroup√©s autour du centre $(\\mathbf{0})$, l'espace est dense et continu, permettant l'interpolation fluide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0c1bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.manifold import TSNE\n",
    "# from torch.utils.data import ConcatDataset\n",
    "\n",
    "# # 1. Extraction des codes latents du VAE\n",
    "# all_latents = []\n",
    "# all_labels = []\n",
    "# vae.eval()\n",
    "# for images, labels in test_dataloader:\n",
    "#     images = images.to(device)\n",
    "#     # On r√©cup√®re la moyenne (mu) comme point latent\n",
    "#     mu, _ = vae.encoder(images)\n",
    "#     all_latents.append(mu.cpu().numpy())\n",
    "#     all_labels.append(labels.numpy())\n",
    "\n",
    "# latents = np.concatenate(all_latents, axis=0)\n",
    "# labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "# # 2. R√©duction de dimension avec t-SNE\n",
    "# tsne = TSNE(n_components=2, random_state=42, verbose=1)\n",
    "# latents_2d = tsne.fit_transform(latents)\n",
    "\n",
    "# # 3. Visualisation\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# scatter = plt.scatter(latents_2d[:, 0], latents_2d[:, 1], c=labels, cmap='viridis', s=10)\n",
    "# plt.colorbar(scatter, label='Classes Fashion-MNIST')\n",
    "# plt.title('Projection 2D des Codes Latents du VAE (via t-SNE)')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
