{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4376de1f",
   "metadata": {
    "id": "intro"
   },
   "source": [
    "# üéì Travaux Pratiques : WGAN-GP (Wasserstein GAN with Gradient Penalty)\n",
    "\n",
    "**Auteur :**Benlahmar Habib\n",
    "\n",
    "\n",
    "## Objectifs P√©dagogiques\n",
    "\n",
    "1.  **Th√©orie du Wasserstein :** Comprendre le passage de la divergence Minimax (BCE) √† la **Distance de Wasserstein** (ou *Earth Mover's Distance*).\n",
    "2.  **Le R√¥le du Critique :** Expliquer pourquoi le Discriminateur devient un **Critique** et ne pr√©dit plus une probabilit√©.\n",
    "3.  **Stabilit√© :** Impl√©menter la **P√©nalit√© de Gradient (GP)** pour respecter la **Contrainte de Lipschitz** et garantir la stabilit√© de l'entra√Ænement.\n",
    "4.  **Boucle d'Entra√Ænement Asym√©trique :** Ma√Ætriser le ratio d'entra√Ænement $n_{\\text{Critique}} : n_{\\text{G√©n√©rateur}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf9bf05",
   "metadata": {
    "id": "concept_wgan"
   },
   "source": [
    "--- \n",
    "\n",
    "## I. Fondements Th√©oriques : Wasserstein et Lipschitz\n",
    "\n",
    "Le GAN classique (bas√© sur BCE) souffre de probl√®mes de **Gradient Vanishing** lorsque les distributions g√©n√©r√©e et r√©elle ne se chevauchent pas (cas fr√©quent). Le WGAN r√©sout ce probl√®me en utilisant la Distance de Wasserstein ($W$), qui fournit un gradient significatif m√™me dans ces conditions difficiles.\n",
    "\n",
    "### 1.1. La Distance de Wasserstein et le Critique\n",
    "\n",
    "La perte du WGAN s'appuie sur la distance de Wasserstein (co√ªt minimal pour transformer une distribution en une autre) :\n",
    "\n",
    "$$\\min_G W(p_{\\text{data}}, p_g) \\approx \\min_G \\max_{C \\in \\mathcal{C}} \\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}} [C(\\mathbf{x})] - \\mathbb{E}_{\\mathbf{z} \\sim p_{\\mathbf{z}}} [C(G(\\mathbf{z}))]$$\n",
    "\n",
    "* **Critique ($C$) :** Mesure la distance entre les distributions. Il ne produit pas une probabilit√©, mais un **score** non born√© (plus le score est grand pour le r√©el et petit pour le faux, plus la distance est grande).\n",
    "\n",
    "### 1.2. La Contrainte de Lipschitz et la P√©nalit√© de Gradient (GP)\n",
    "\n",
    "Pour que $C$ soit une approximation valide de la distance $W$, il doit √™tre une fonction **$1$-Lipschitz** : la norme de son gradient doit √™tre inf√©rieure ou √©gale √† 1 partout ($\\left\\|\\nabla_{\\mathbf{x}} C(\\mathbf{x})\\right\\| \\le 1$).\n",
    "\n",
    "Le WGAN-GP applique la P√©nalit√© de Gradient ($GP$) pour forcer cette contrainte. Le terme est ajout√© √† la perte du Critique :\n",
    "\n",
    "$$\\text{P√©nalit√© de Gradient} = \\lambda \\cdot \\mathbb{E}_{\\hat{\\mathbf{x}}} [ (\\| \\nabla_{\\hat{\\mathbf{x}}} C(\\hat{\\mathbf{x}}) \\|_2 - 1)^2 ]$$\n",
    "\n",
    "### Question d'Accompagnement (Q1.1)\n",
    "\n",
    "Quel est le r√¥le du **Critique ($C$)** dans le WGAN-GP, et en quoi diff√®re-t-il du r√¥le de classification binaire du Discriminateur dans le GAN classique ? (Indice : Que repr√©sentent les scores de sortie du Critique ?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0213ec5a",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "--- \n",
    "\n",
    "## II. Configuration et Architecture (DCGAN comme base)\n",
    "\n",
    "Nous utilisons l'architecture DCGAN pour le G√©n√©rateur et le Critique, en ajustant les hyperparam√®tres et la normalisation des donn√©es √† $[-1, 1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e89485",
   "metadata": {
    "id": "imports_wgangp"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Ex√©cution sur {device}\")\n",
    "\n",
    "# Hyperparam√®tres WGAN-GP\n",
    "latent_dim = 100\n",
    "batch_size = 128\n",
    "epochs = 100\n",
    "lr = 0.0001 # Taux d'apprentissage plus bas que le GAN classique\n",
    "beta1 = 0.5 # Param√®tre Adam\n",
    "lambda_gp = 10 # Coefficient de la P√©nalit√© de Gradient (standard = 10)\n",
    "n_critic = 5 # Ratio C:G, entra√Æne C 5 fois pour 1 entra√Ænement de G\n",
    "\n",
    "# NORMALISATION : [0, 1] -> [-1, 1] pour Tanh\n",
    "transform_dcgan = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.5,), (0.5,)) \n",
    "])\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root='./data/FashionMNIST', train=True, download=True, transform=transform_dcgan)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialisation des poids (selon les recommandations DCGAN)\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        init.constant_(m.bias.data, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555711ec",
   "metadata": {
    "id": "arch_dcgan_critic"
   },
   "source": [
    "### 2.1. Le Critique ($C$)\n",
    "\n",
    "Architecture DCGAN sans `Sigmoid` ni `BatchNorm` sur la sortie, car il renvoie un score r√©el non born√©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9e6bc5",
   "metadata": {
    "id": "critic_dcgan"
   },
   "outputs": [],
   "source": [
    "ndf = 64 # Nombre de 'Discriminator Feature maps'\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # 1 x 28 x 28 -> ndf x 14 x 14 (Pas de BN sur la premi√®re couche)\n",
    "            nn.Conv2d(1, ndf, 4, 2, 1, bias=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # ndf x 14 x 14 -> ndf*2 x 7 x 7\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=True), \n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # ndf*2 x 7 x 7 -> ndf*4 x 4 x 4\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 3, 2, 1, bias=True), \n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # Sortie : ndf*4 x 4 x 4 -> 1 (score brut)\n",
    "            nn.Conv2d(ndf * 4, 1, 4, 1, 0, bias=True), \n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input).view(-1, 1)\n",
    "\n",
    "### 2.2. Le G√©n√©rateur ($G$) (Identique au DCGAN)\n",
    "\n",
    "ngf = 64\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        self.ngf = ngf\n",
    "        self.main = nn.Sequential(\n",
    "            # Projection initiale du bruit z (100) en volume spatial (ngf*4 x 4 x 4)\n",
    "            nn.ConvTranspose2d(latent_dim, self.ngf * 4, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(self.ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(self.ngf * 4, self.ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(self.ngf * 2, self.ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.ngf),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # Sortie : Tanh pour [-1, 1]\n",
    "            nn.ConvTranspose2d(self.ngf, 1, 4, 2, 3, bias=False), \n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.main(z.view(-1, latent_dim, 1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9db753",
   "metadata": {
    "id": "q2"
   },
   "source": [
    "### Question d'Accompagnement (Q2.1)\n",
    "\n",
    "Pourquoi est-il n√©cessaire de modifier le ratio d'entra√Ænement pour que $n_{\\text{Critique}} > n_{\\text{G√©n√©rateur}}$ (par exemple, 5:1), alors que le GAN classique utilisait un ratio 1:1 ? (Indice : Pensez √† la stabilit√© et √† la n√©cessit√© que le Critique soit pr√©cis pour fournir un gradient utile.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1de791",
   "metadata": {
    "id": "gp_function"
   },
   "source": [
    "--- \n",
    "\n",
    "## III. Impl√©mentation du Gradient Penalty (GP)\n",
    "\n",
    "Cette fonction calcule le terme de p√©nalit√© en s'assurant que la norme du gradient du Critique est proche de 1 sur les points interpol√©s entre les distributions r√©elle et fausse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1960e62a",
   "metadata": {
    "id": "gp_code"
   },
   "outputs": [],
   "source": [
    "def calculate_gradient_penalty(C, real_images, fake_images, lambda_gp, device):\n",
    "    b_size = real_images.size(0)\n",
    "    # 1. G√©n√©ration des coefficients al√©atoires alpha\n",
    "    alpha = torch.rand(b_size, 1, 1, 1, device=device)\n",
    "\n",
    "    # 2. √âchantillonnage interpol√© : x_hat = alpha * real + (1 - alpha) * fake\n",
    "    x_hat = alpha * real_images + (1 - alpha) * fake_images\n",
    "    x_hat.requires_grad_(True)\n",
    "\n",
    "    # 3. Score du Critique sur les √©chantillons interpol√©s\n",
    "    C_x_hat = C(x_hat)\n",
    "\n",
    "    # 4. Calcul du gradient du Critique par rapport √† x_hat\n",
    "    gradients = torch.autograd.grad(outputs=C_x_hat,\n",
    "                                    inputs=x_hat,\n",
    "                                    grad_outputs=torch.ones_like(C_x_hat),\n",
    "                                    create_graph=True,\n",
    "                                    retain_graph=True)[0]\n",
    "\n",
    "    # 5. Calcul de la norme du gradient et de la P√©nalit√©\n",
    "    gradients = gradients.view(b_size, -1)\n",
    "    gradient_norm = gradients.norm(2, dim=1)\n",
    "\n",
    "    # P√©nalit√© : (||grad|| - 1)^2\n",
    "    gp = lambda_gp * ((gradient_norm - 1)**2).mean()\n",
    "    return gp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3b06e5",
   "metadata": {
    "id": "q3"
   },
   "source": [
    "### Question d'Accompagnement (Q3.1)\n",
    "\n",
    "Pourquoi est-il n√©cessaire d'√©chantillonner des points $\\hat{\\mathbf{x}}$ **entre** les distributions r√©elle et fausse pour calculer la P√©nalit√© de Gradient, plut√¥t que de simplement calculer la p√©nalit√© sur les images r√©elles et fausses s√©par√©ment ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb58052",
   "metadata": {
    "id": "loop"
   },
   "source": [
    "--- \n",
    "\n",
    "## IV. Boucle d'Entra√Ænement WGAN-GP\n",
    "\n",
    "La boucle d'entra√Ænement doit respecter le ratio $n_{\\text{critique}} : 1$ et int√©grer la P√©nalit√© de Gradient √† la perte du Critique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3369dbb2",
   "metadata": {
    "id": "train_wgangp"
   },
   "outputs": [],
   "source": [
    "# Initialisation\n",
    "G = Generator(latent_dim).to(device).apply(weights_init)\n",
    "C = Critic().to(device).apply(weights_init)\n",
    "\n",
    "# Optimiseurs (Adam avec betas ajust√©s)\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "C_optimizer = optim.Adam(C.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "fixed_noise = torch.randn(64, latent_dim, device=device)\n",
    "\n",
    "# Fonction utilitaire de visualisation\n",
    "def show_grid(grid, title=\"\", figsize=(10, 10)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.title(title)\n",
    "    grid = (grid + 1) / 2 # Inverse la normalisation pour l'affichage : [-1, 1] -> [0, 1]\n",
    "    plt.imshow(np.transpose(grid.numpy(), (1, 2, 0)), cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "def train_wgangp(G, C, G_optimizer, C_optimizer, dataloader, epochs, latent_dim, device, lambda_gp, n_critic):\n",
    "    \n",
    "    for epoch in trange(epochs, desc=\"Entra√Ænement WGAN-GP\"):\n",
    "        for i, (real_images, _) in enumerate(dataloader):\n",
    "            \n",
    "            real_images = real_images.to(device)\n",
    "            b_size = real_images.size(0)\n",
    "            \n",
    "            ############################\n",
    "            # (1) Mise √† jour C (Critique) : n_critic fois\n",
    "            ############################\n",
    "            for _ in range(n_critic):\n",
    "                C_optimizer.zero_grad()\n",
    "                \n",
    "                # 1a. G√©n√©ration d'images fausses\n",
    "                noise = torch.randn(b_size, latent_dim, device=device)\n",
    "                fake_images = G(noise).detach() \n",
    "                \n",
    "                # 1b. Scores du Critique\n",
    "                C_real = C(real_images)\n",
    "                C_fake = C(fake_images)\n",
    "                \n",
    "                # 1c. Calcul de la Perte de Wasserstein \n",
    "                # C_real.mean() - C_fake.mean() donne la W_distance. C maximise cette valeur.\n",
    "                W_distance = C_real.mean() - C_fake.mean()\n",
    "                C_W_loss = -W_distance # Pour la minimisation avec l'optimiseur (Loss = -W_distance)\n",
    "                \n",
    "                # 1d. Calcul de la P√©nalit√© de Gradient (GP)\n",
    "                gp = calculate_gradient_penalty(C, real_images, fake_images, lambda_gp, device)\n",
    "                \n",
    "                # 1e. Perte totale du Critique\n",
    "                C_loss = C_W_loss + gp\n",
    "                \n",
    "                C_loss.backward()\n",
    "                C_optimizer.step()\n",
    "                \n",
    "            ############################\n",
    "            # (2) Mise √† jour G (G√©n√©rateur) : 1 fois\n",
    "            ############################\n",
    "            G.zero_grad()\n",
    "            \n",
    "            # 2a. Score du Critique sur les fausses images\n",
    "            noise = torch.randn(b_size, latent_dim, device=device)\n",
    "            fake_images = G(noise)\n",
    "            C_fake = C(fake_images)\n",
    "            \n",
    "            # 2b. Perte de G : G tente de MAXIMISER le score des fausses images\n",
    "            G_loss = -C_fake.mean() # Minimiser -Score(Faux) = Maximiser Score(Faux)\n",
    "            \n",
    "            G_loss.backward()\n",
    "            G_optimizer.step()\n",
    "            \n",
    "        # Affichage de l'√©volution (par √©poque)\n",
    "        tqdm.write(f\"Epoch {epoch+1:2d} | C Loss: {C_loss.item():.4f} | W Dist: {W_distance.item():.4f} | G Loss: {G_loss.item():.4f}\")\n",
    "        \n",
    "        # 3. Visualisation\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            G.eval()\n",
    "            with torch.no_grad():\n",
    "                generated_images = G(fixed_noise).cpu()\n",
    "                show_grid(make_grid(generated_images, 8), title=f\"WGAN-GP G√©n√©ration √âpoque {epoch+1}\")\n",
    "            G.train()\n",
    "\n",
    "# train_wgangp(G, C, G_optimizer, C_optimizer, train_dataloader, epochs, latent_dim, device, lambda_gp, n_critic) # <-- D√âCOMMENTER POUR LANCER L'ENTRAINEMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ed51c5",
   "metadata": {
    "id": "synthesis"
   },
   "source": [
    "--- \n",
    "\n",
    "## V. Synth√®se et Ouverture (Post-Entra√Ænement)\n",
    "\n",
    "### Questions Finales\n",
    "\n",
    "1.  **Interpr√©tabilit√© de la M√©trique :** Que repr√©sente la valeur `W Dist` affich√©e √† la fin de chaque √©poque, et pourquoi est-elle consid√©r√©e comme une **meilleure m√©trique** de convergence et de qualit√© que la BCE Loss des GANs classiques ?\n",
    "2.  **R√¥le de $\\lambda_{GP}$ :** Le coefficient de p√©nalit√© $\\lambda_{GP}$ est fix√© √† 10. Que se passerait-il si vous fixiez $\\lambda_{GP}$ √† une valeur tr√®s √©lev√©e (par exemple 100) ? Quel serait l'impact sur les scores du Critique et sur la rapidit√© de convergence du G√©n√©rateur ?\n",
    "3.  **Comparaison de Stabilit√© :** D√©crivez comment la boucle d'entra√Ænement du WGAN-GP (avec $n_{\\text{critique}} > 1$ et GP) r√©duit les risques de **Mode Collapse** par rapport au DCGAN classique.\n",
    "4.  **Prochaines √âtapes :** Le WGAN-GP a grandement am√©lior√© la stabilit√© des GANs. Pour aller plus loin dans la qualit√© et la **manipulation s√©mantique** des images, quel type d'architecture moderne (par exemple, **StyleGAN**) int√®gre le WGAN-GP tout en se concentrant sur la manipulation du bruit latent √† diff√©rentes √©chelles de l'image ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
