{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e41666d",
   "metadata": {
    "id": "intro_stylegan"
   },
   "source": [
    "# üéì Travaux Pratiques : StyleGAN (G√©n√©ration Contr√¥l√©e Simplifi√©e)\n",
    "\n",
    "**Auteur :**Benlahmar Habib\n",
    "\n",
    "\n",
    "## Objectifs P√©dagogiques\n",
    "\n",
    "1.  **Mapping Network :** Comprendre l'avantage de s√©parer l'espace latent initial ($z$) de l'espace de style interm√©diaire ($w$).\n",
    "2.  **Adaptive Instance Normalization (AdaIN) :** Impl√©menter et comprendre comment cette couche module le style et la texture.\n",
    "3.  **Contr√¥labilit√© :** Observer comment le style est appliqu√© localement √† chaque niveau de r√©solution du G√©n√©rateur.\n",
    "4.  **Base Stabilit√© :** Utiliser les principes du WGAN-GP pour garantir la stabilit√© de ce nouveau G√©n√©rateur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981745f6",
   "metadata": {
    "id": "concept_stylegan"
   },
   "source": [
    "--- \n",
    "\n",
    "## I. Fondements Th√©oriques : Style et S√©paration\n",
    "\n",
    "Les GANs pr√©c√©dents souffraient de l'*enchev√™trement latent* : une dimension du bruit $z$ pouvait affecter simultan√©ment la forme, la couleur et la texture. Le StyleGAN r√©sout ce probl√®me en introduisant un **Mapping Network** pour cr√©er un espace de style ($w$) *d√©-enchev√™tr√©* et en injectant ce style via **AdaIN** √† chaque couche.\n",
    "\n",
    "### 1.1. Adaptive Instance Normalization (AdaIN)\n",
    "\n",
    "AdaIN remplace la Batch Normalization dans le G√©n√©rateur. Pour une activation $\\mathbf{x}$ et un vecteur de style $w$ (qui fournit les param√®tres de mise √† l'√©chelle $\\mathbf{y}_s$ et de d√©calage $\\mathbf{y}_b$), l'op√©ration est :\n",
    "\n",
    "$$\\text{AdaIN}(\\mathbf{x}, \\mathbf{y}) = \\mathbf{y}_s \\frac{\\mathbf{x} - \\mu(\\mathbf{x})}{\\sigma(\\mathbf{x})} + \\mathbf{y}_b$$\n",
    "\n",
    "### Question d'Accompagnement (Q1.1)\n",
    "\n",
    "En quoi l'utilisation de l'**Instance Normalization** (qui normalise chaque √©chantillon ind√©pendamment du lot, $\\mu(\\mathbf{x}), \\sigma(\\mathbf{x})$) au lieu de la **Batch Normalization** (qui utilise $\\mu(\\text{batch}), \\sigma(\\text{batch})$) est-elle essentielle pour que la couche AdaIN puisse injecter le style de mani√®re localis√©e et efficace ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b723467",
   "metadata": {
    "id": "setup_stylegan"
   },
   "source": [
    "--- \n",
    "\n",
    "## II. Configuration et Architecture StyleGAN Simplifi√©e\n",
    "\n",
    "Nous utilisons le WGAN-GP pour la perte du Critique et d√©finissons les nouveaux modules du G√©n√©rateur : AdaIN, Mapping Network et Synthesis Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5397f29d",
   "metadata": {
    "id": "imports_stylegan"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ex√©cution sur cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Ex√©cution sur {device}\")\n",
    "\n",
    "# Hyperparam√®tres WGAN-GP (R√©utilis√©s)\n",
    "latent_dim = 100 # Dimension de z (bruit initial)\n",
    "style_dim = 100  # Dimension de w (style)\n",
    "batch_size = 128\n",
    "epochs = 100\n",
    "lr = 0.0001\n",
    "beta1 = 0.5\n",
    "lambda_gp = 10 \n",
    "n_critic = 5 \n",
    "ngf = 64 # Nombre de feature maps du g√©n√©rateur\n",
    "ndf = 64 # Nombre de feature maps du discriminateur\n",
    "\n",
    "# NORMALISATION : [0, 1] -> [-1, 1]\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.5,), (0.5,)) \n",
    "])\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root='./data/FashionMNIST', train=True, download=True, transform=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# --- 2.1. Module AdaIN ---\n",
    "class AdaptiveInstanceNorm(nn.Module):\n",
    "    def __init__(self, num_features, style_dim):\n",
    "        super().__init__()\n",
    "        # Projette le style w en param√®tres y_s et y_b\n",
    "        self.style_scale = nn.Linear(style_dim, num_features)\n",
    "        self.style_bias = nn.Linear(style_dim, num_features)\n",
    "\n",
    "        self.instance_norm = nn.InstanceNorm2d(num_features, affine=False) \n",
    "\n",
    "    def forward(self, x, w):\n",
    "        # Calcul des param√®tres de style, format√©s pour l'op√©ration (1, C, 1, 1)\n",
    "        y_s = self.style_scale(w).unsqueeze(-1).unsqueeze(-1)\n",
    "        y_b = self.style_bias(w).unsqueeze(-1).unsqueeze(-1)\n",
    "        \n",
    "        # Applique l'Instance Normalization et le Style (y_s * x + y_b)\n",
    "        x = self.instance_norm(x)\n",
    "        return y_s * x + y_b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f7e750",
   "metadata": {
    "id": "mapping_network"
   },
   "source": [
    "### 2.2. Mapping Network ($M$) et Synthesis Network ($G_{\\text{synth}}$)\n",
    "\n",
    "Le **Mapping Network** cr√©e le style $w$. Le **Synthesis Network** utilise ce style pour construire l'image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246ec674",
   "metadata": {
    "id": "synthesis_code"
   },
   "outputs": [],
   "source": [
    "class MappingNetwork(nn.Module):\n",
    "    def __init__(self, latent_dim, style_dim, num_layers=4):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_dim = latent_dim\n",
    "        for i in range(num_layers):\n",
    "            layers.append(nn.Linear(in_dim, style_dim))\n",
    "            layers.append(nn.LeakyReLU(0.2))\n",
    "            in_dim = style_dim\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "class SynthesisBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, style_dim, upsample=True):\n",
    "        super().__init__()\n",
    "        # Simplification : utilise nn.Upsample + Conv au lieu de ConvTranspose2d\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest') if upsample else None\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.adain = AdaptiveInstanceNorm(out_channels, style_dim)\n",
    "        self.lrelu = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x, w):\n",
    "        if self.upsample: x = self.upsample(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.adain(x, w)\n",
    "        x = self.lrelu(x)\n",
    "        return x\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, style_dim):\n",
    "        super().__init__()\n",
    "        self.mapping = MappingNetwork(latent_dim, style_dim)\n",
    "        \n",
    "        # Input constant (remplace l'entr√©e spatiale du bruit)\n",
    "        self.const_input = nn.Parameter(torch.randn(1, ngf * 4, 4, 4))\n",
    "        \n",
    "        self.synth = nn.Sequential(\n",
    "            SynthesisBlock(ngf * 4, ngf * 4, style_dim, upsample=False), # 4x4\n",
    "            SynthesisBlock(ngf * 4, ngf * 2, style_dim, upsample=True), # 8x8 \n",
    "            SynthesisBlock(ngf * 2, ngf, style_dim, upsample=True), # 16x16\n",
    "            SynthesisBlock(ngf, ngf, style_dim, upsample=True), # 32x32\n",
    "        )\n",
    "        \n",
    "        self.to_rgb = nn.Conv2d(ngf, 1, kernel_size=1) \n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, z):\n",
    "        w = self.mapping(z) # Style vector\n",
    "        \n",
    "        # R√©p√©ter la constante pour le batch\n",
    "        x = self.const_input.repeat(z.size(0), 1, 1, 1) \n",
    "        \n",
    "        # Passage dans le Synthesis Network\n",
    "        for module in self.synth:\n",
    "            if isinstance(module, SynthesisBlock):\n",
    "                x = module(x, w) \n",
    "            else:\n",
    "                x = module(x)\n",
    "        \n",
    "        x = self.to_rgb(x)\n",
    "        # Cropping 32x32 -> 28x28 pour Fashion-MNIST (pour simplifier le r√©seau)\n",
    "        x = x[:, :, 2:30, 2:30] \n",
    "        return self.tanh(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52030279",
   "metadata": {
    "id": "q2"
   },
   "source": [
    "### Question d'Accompagnement (Q2.1)\n",
    "\n",
    "Dans l'architecture StyleGAN, l'entr√©e bruit $z$ n'est utilis√©e que dans le **Mapping Network** pour cr√©er le style $w$. Quelle est l'autre source de bruit introduite directement dans le **Synthesis Network** (dans le StyleGAN complet) ? Quel est l'objectif de ce second bruit ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd98399",
   "metadata": {
    "id": "loop_stylegan_wgan"
   },
   "source": [
    "--- \n",
    "\n",
    "## III. Boucle d'Entra√Ænement (WGAN-GP pour la Stabilit√©)\n",
    "\n",
    "Nous utilisons le Critique DCGAN et la perte WGAN-GP pour la stabilit√©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742ed8a6",
   "metadata": {
    "id": "discriminator_wgangp"
   },
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(1, ndf, 4, 2, 1, bias=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=True), \n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 3, 2, 1, bias=True), \n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ndf * 4, 1, 4, 1, 0, bias=True), \n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input).view(-1, 1)\n",
    "\n",
    "# Fonction de P√©nalit√© de Gradient (r√©utilis√©e)\n",
    "def calculate_gradient_penalty(C, real_images, fake_images, lambda_gp, device):\n",
    "    b_size = real_images.size(0)\n",
    "    alpha = torch.rand(b_size, 1, 1, 1, device=device)\n",
    "    x_hat = alpha * real_images + (1 - alpha) * fake_images\n",
    "    x_hat.requires_grad_(True)\n",
    "    C_x_hat = C(x_hat)\n",
    "\n",
    "    gradients = torch.autograd.grad(outputs=C_x_hat,\n",
    "                                    inputs=x_hat,\n",
    "                                    grad_outputs=torch.ones_like(C_x_hat),\n",
    "                                    create_graph=True,\n",
    "                                    retain_graph=True)[0]\n",
    "\n",
    "    gradients = gradients.view(b_size, -1)\n",
    "    gradient_norm = gradients.norm(2, dim=1)\n",
    "    gp = lambda_gp * ((gradient_norm - 1)**2).mean()\n",
    "    return gp\n",
    "\n",
    "# Initialisation\n",
    "G = Generator(latent_dim, style_dim).to(device)\n",
    "C = Critic().to(device)\n",
    "\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "C_optimizer = optim.Adam(C.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "fixed_noise = torch.randn(64, latent_dim, device=device)\n",
    "\n",
    "# Fonction de visualisation\n",
    "def show_grid(grid, title=\"\", figsize=(10, 10)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.title(title)\n",
    "    grid = (grid + 1) / 2 \n",
    "    plt.imshow(np.transpose(grid.numpy(), (1, 2, 0)), cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "def train_stylegan_wgangp(G, C, G_optimizer, C_optimizer, dataloader, epochs, latent_dim, device, lambda_gp, n_critic):\n",
    "    \n",
    "    for epoch in trange(epochs, desc=\"Entra√Ænement StyleGAN/WGAN-GP\"):\n",
    "        for i, (real_images, _) in enumerate(dataloader):\n",
    "            \n",
    "            real_images = real_images.to(device)\n",
    "            b_size = real_images.size(0)\n",
    "            \n",
    "            # (1) Mise √† jour C (Critique) : n_critic fois\n",
    "            for _ in range(n_critic):\n",
    "                C_optimizer.zero_grad()\n",
    "                \n",
    "                noise = torch.randn(b_size, latent_dim, device=device)\n",
    "                fake_images = G(noise).detach() \n",
    "                \n",
    "                # Loss WGAN\n",
    "                C_real = C(real_images)\n",
    "                C_fake = C(fake_images)\n",
    "                W_distance = C_real.mean() - C_fake.mean()\n",
    "                C_W_loss = -W_distance \n",
    "                \n",
    "                # P√©nalit√© de Gradient\n",
    "                gp = calculate_gradient_penalty(C, real_images, fake_images, lambda_gp, device)\n",
    "                C_loss = C_W_loss + gp\n",
    "                \n",
    "                C_loss.backward()\n",
    "                C_optimizer.step()\n",
    "                \n",
    "            # (2) Mise √† jour G (G√©n√©rateur) : 1 fois\n",
    "            G_optimizer.zero_grad()\n",
    "            \n",
    "            noise = torch.randn(b_size, latent_dim, device=device)\n",
    "            fake_images = G(noise)\n",
    "            C_fake = C(fake_images)\n",
    "            \n",
    "            G_loss = -C_fake.mean()\n",
    "            \n",
    "            G_loss.backward()\n",
    "            G_optimizer.step()\n",
    "            \n",
    "        tqdm.write(f\"Epoch {epoch+1:2d} | C Loss: {C_loss.item():.4f} | W Dist: {W_distance.item():.4f} | G Loss: {G_loss.item():.4f}\")\n",
    "        \n",
    "        # 3. Visualisation\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            G.eval()\n",
    "            with torch.no_grad():\n",
    "                generated_images = G(fixed_noise).cpu()\n",
    "                show_grid(make_grid(generated_images, 8), title=f\"StyleGAN G√©n√©ration √âpoque {epoch+1}\")\n",
    "            G.train()\n",
    "\n",
    "# train_stylegan_wgangp(G, C, G_optimizer, C_optimizer, train_dataloader, epochs, latent_dim, device, lambda_gp, n_critic) # <-- D√âCOMMENTER POUR LANCER L'ENTRAINEMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ef928b",
   "metadata": {
    "id": "synthesis"
   },
   "source": [
    "--- \n",
    "\n",
    "## V. Synth√®se et Ouverture (Post-Entra√Ænement)\n",
    "\n",
    "### Questions Finales\n",
    "\n",
    "1.  **AdaIN vs. BN :** Expliquez la principale diff√©rence entre la Batch Normalization (BN) utilis√©e dans le DCGAN et l'Adaptive Instance Normalization (AdaIN) utilis√©e ici. Quel type d'information chaque m√©thode normalise-t-elle, et pourquoi AdaIN permet-il d'injecter des *styles* ?\n",
    "2.  **Enc√™trement Latent :** Quel probl√®me th√©orique majeur le **Mapping Network** ($z \\to w$) tente-t-il de r√©soudre par rapport √† l'utilisation directe de $z$ comme entr√©e ? Quel est le lien avec la facilit√© de la manipulation s√©mantique des images ?\n",
    "3.  **H√©t√©rog√©n√©it√© des Styles :** Le StyleGAN complet injecte diff√©rents vecteurs de style ($w_1, w_2, ...$) √† diff√©rentes r√©solutions du r√©seau. Expliquez comment l'injection de $w_{\\text{basse r√©solution}}$ peut influencer la *forme* g√©n√©rale de l'objet, tandis que $w_{\\text{haute r√©solution}}$ influence la *texture* et les *d√©tails*.\n",
    "4.  **Au-del√† de StyleGAN :** Le StyleGAN a √©t√© suivi par StyleGAN2 et StyleGAN3. Citez une des principales am√©liorations du StyleGAN2 (Indice : Normalisation et artefacts de gouttelette)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
