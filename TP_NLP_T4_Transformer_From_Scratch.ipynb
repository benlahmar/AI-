{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6712db42",
   "metadata": {},
   "source": [
    "# TP NLP ‚Äî T4 : **Transformer from Scratch** (Self-Attention) ‚Äî Master IA\n",
    "\n",
    "Ce notebook constitue le **TUTORIEL 4 (T4)** du module NLP.\n",
    "Apr√®s Seq2Seq (T1), Bi-Encodeur (T2) et Attention (T3), on introduit le **Transformer**,\n",
    "architecture fond√©e **uniquement sur l‚Äôattention**, sans r√©currence.\n",
    "\n",
    "---\n",
    "## üéØ Objectifs p√©dagogiques\n",
    "\n",
    "√Ä la fin de ce TP, l‚Äô√©tudiant sera capable de :\n",
    "- expliquer pourquoi le Transformer **supprime la r√©currence**,\n",
    "- comprendre la **self-attention** (requ√™tes, cl√©s, valeurs),\n",
    "- impl√©menter une **Multi-Head Attention** simple,\n",
    "- comprendre le r√¥le du **positional encoding**,\n",
    "- assembler un **mini-Transformer fonctionnel**,\n",
    "- comparer conceptuellement Transformer vs RNN + Attention.\n",
    "\n",
    "‚ö†Ô∏è Objectif : **comprendre l‚Äôarchitecture**, pas battre l‚Äô√©tat de l‚Äôart.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc606aa",
   "metadata": {},
   "source": [
    "\n",
    "## üß† Pourquoi les Transformers ? (rappel critique)\n",
    "\n",
    "Les mod√®les RNN/LSTM pr√©sentent :\n",
    "- une **s√©quentialit√© stricte** (pas de parall√©lisme),\n",
    "- des difficult√©s sur les longues d√©pendances,\n",
    "- un co√ªt temporel proportionnel √† la longueur.\n",
    "\n",
    "Le Transformer (Vaswani et al., 2017) repose sur une id√©e radicale :\n",
    "> **‚ÄúAttention is all you need.‚Äù**\n",
    "\n",
    "‚û°Ô∏è Toutes les positions d‚Äôune s√©quence interagissent **en parall√®le**.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22ac742",
   "metadata": {},
   "source": [
    "\n",
    "## üß© Probl√®me p√©dagogique choisi\n",
    "\n",
    "Nous conservons le probl√®me **d‚Äôinversion de s√©quence** :\n",
    "```\n",
    "[1, 5, 7, 3] ‚Üí [3, 7, 5, 1]\n",
    "```\n",
    "\n",
    "Pourquoi ?\n",
    "- structure s√©quence‚Üís√©quence,\n",
    "- comparaison directe avec T1‚ÄìT3,\n",
    "- interpr√©tabilit√© claire de la self-attention.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc7df41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3a85ff",
   "metadata": {},
   "source": [
    "## 1) Param√®tres et vocabulaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68943e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "V = 20\n",
    "MIN_LEN, MAX_LEN = 3, 12\n",
    "\n",
    "TRAIN_SIZE = 8000\n",
    "VALID_SIZE = 1000\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "D_MODEL = 128\n",
    "N_HEADS = 4\n",
    "FF_DIM = 256\n",
    "\n",
    "EPOCHS = 10\n",
    "LR = 1e-3\n",
    "\n",
    "PAD = 0\n",
    "SOS = V + 1\n",
    "EOS = V + 2\n",
    "VOCAB_SIZE = V + 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6b82d2",
   "metadata": {},
   "source": [
    "## 2) Dataset (inversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db41a267",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_pair():\n",
    "    L = random.randint(MIN_LEN, MAX_LEN)\n",
    "    src = [random.randint(1, V) for _ in range(L)]\n",
    "    tgt = [SOS] + list(reversed(src)) + [EOS]\n",
    "    return src, tgt\n",
    "\n",
    "class ReverseDataset(Dataset):\n",
    "    def __init__(self, n):\n",
    "        self.data = [generate_pair() for _ in range(n)]\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self, i): return self.data[i]\n",
    "\n",
    "def pad(seqs):\n",
    "    m = max(len(s) for s in seqs)\n",
    "    return torch.tensor([s+[PAD]*(m-len(s)) for s in seqs], dtype=torch.long)\n",
    "\n",
    "def collate(batch):\n",
    "    src = pad([b[0] for b in batch])\n",
    "    tgt = pad([b[1] for b in batch])\n",
    "    return src, tgt[:,:-1], tgt[:,1:]\n",
    "\n",
    "train_loader = DataLoader(ReverseDataset(TRAIN_SIZE), batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate)\n",
    "valid_loader = DataLoader(ReverseDataset(VALID_SIZE), batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdd47bf",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Positional Encoding\n",
    "\n",
    "La self-attention est **invariante √† l‚Äôordre**.\n",
    "On ajoute donc une information de position via :\n",
    "\\[\n",
    "PE(pos,2i)=sin(pos/10000^{2i/d})\n",
    "\\]\n",
    "\\[\n",
    "PE(pos,2i+1)=cos(pos/10000^{2i/d})\n",
    "\\]\n",
    "\n",
    "‚û°Ô∏è Cela injecte la notion d‚Äôordre sans r√©currence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb380cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=100):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8431215",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Scaled Dot-Product Attention\n",
    "\n",
    "Pour des requ√™tes Q, cl√©s K et valeurs V :\n",
    "\\[\n",
    "Attention(Q,K,V)=softmax(QK^T/\\sqrt{d_k})V\n",
    "\\]\n",
    "\n",
    "Le facteur \\(\\sqrt{d_k}\\) stabilise les gradients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe0c8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ScaledDotAttention(nn.Module):\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        d_k = Q.size(-1)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        return torch.matmul(attn, V), attn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f91ada",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Multi-Head Attention\n",
    "\n",
    "On projette Q,K,V en **plusieurs sous-espaces** :\n",
    "- chaque t√™te apprend un type de relation\n",
    "- les r√©sultats sont concat√©n√©s puis reprojet√©s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c877a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.Wq = nn.Linear(d_model, d_model)\n",
    "        self.Wk = nn.Linear(d_model, d_model)\n",
    "        self.Wv = nn.Linear(d_model, d_model)\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        self.attn = ScaledDotAttention()\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, T, D = x.shape\n",
    "        Q = self.Wq(x).view(B, T, self.n_heads, self.d_k).transpose(1,2)\n",
    "        K = self.Wk(x).view(B, T, self.n_heads, self.d_k).transpose(1,2)\n",
    "        V = self.Wv(x).view(B, T, self.n_heads, self.d_k).transpose(1,2)\n",
    "\n",
    "        out, attn = self.attn(Q, K, V, mask)\n",
    "        out = out.transpose(1,2).contiguous().view(B, T, D)\n",
    "        return self.fc(out), attn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bf9ac1",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Feed-Forward Network position-wise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2cbc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, ff_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, d_model)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4eaba8",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Bloc Transformer (Encoder)\n",
    "\n",
    "Chaque bloc :\n",
    "1. Multi-head self-attention\n",
    "2. Add & Norm\n",
    "3. Feed-forward\n",
    "4. Add & Norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db51e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, ff_dim):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ff = FeedForward(d_model, ff_dim)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_out, attn = self.attn(x, mask)\n",
    "        x = self.norm1(x + attn_out)\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + ff_out)\n",
    "        return x, attn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa492b3b",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Mini-Transformer Seq2Seq (encodeur seul)\n",
    "\n",
    "Pour simplifier :\n",
    "- on utilise **un seul bloc Transformer**\n",
    "- on pr√©dit directement la s√©quence invers√©e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6164932d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MiniTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(VOCAB_SIZE, D_MODEL, padding_idx=PAD)\n",
    "        self.pe = PositionalEncoding(D_MODEL)\n",
    "        self.block = TransformerBlock(D_MODEL, N_HEADS, FF_DIM)\n",
    "        self.fc = nn.Linear(D_MODEL, VOCAB_SIZE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = (x != PAD).unsqueeze(1).unsqueeze(2)\n",
    "        x = self.pe(self.emb(x))\n",
    "        out, attn = self.block(x, mask)\n",
    "        return self.fc(out), attn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d46204",
   "metadata": {},
   "source": [
    "## 9) Entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad54705",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = MiniTransformer().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD)\n",
    "\n",
    "def run_epoch(loader, train=True):\n",
    "    model.train() if train else model.eval()\n",
    "    total = 0\n",
    "    for src, tgt_in, tgt_out in loader:\n",
    "        src, tgt_out = src.to(device), tgt_out.to(device)\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "        logits, _ = model(src)\n",
    "        B,T,V = logits.shape\n",
    "        loss = criterion(logits.view(B*T, V), tgt_out.view(B*T))\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        total += loss.item()\n",
    "    return total/len(loader)\n",
    "\n",
    "for e in range(1, EPOCHS+1):\n",
    "    tr = run_epoch(train_loader, True)\n",
    "    va = run_epoch(valid_loader, False)\n",
    "    print(f\"Epoch {e:02d} | train {tr:.4f} | valid {va:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4049415",
   "metadata": {},
   "source": [
    "\n",
    "## 10) Visualisation de la self-attention\n",
    "\n",
    "On observe les relations apprises entre positions :\n",
    "- diagonale invers√©e attendue pour l‚Äôinversion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1960d89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def show_self_attention(model, seq):\n",
    "    model.eval()\n",
    "    x = torch.tensor([seq], dtype=torch.long, device=device)\n",
    "    _, attn = model(x)\n",
    "    attn = attn[0,0].cpu().numpy()  # t√™te 0\n",
    "    plt.imshow(attn)\n",
    "    plt.colorbar()\n",
    "    plt.title(\"Self-Attention (head 0)\")\n",
    "    plt.xlabel(\"Positions source\")\n",
    "    plt.ylabel(\"Positions source\")\n",
    "    plt.show()\n",
    "\n",
    "show_self_attention(model, [1,5,7,3])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
